{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787b7284",
   "metadata": {},
   "source": [
    "# HT 2 : –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ DistilBERT\n",
    "---\n",
    "–ó–∞–¥–∞—á–∞: —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤¬†2 —Ä–∞–∑–∞ —Å¬†–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534d1be",
   "metadata": {},
   "source": [
    "### üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cb7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa310d1",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch, os\n",
    "\n",
    "MODEL_ID = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"test\", )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eede46",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f7f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I loved this movie, it was fantastic!\n",
      "Prediction: [{'label': 'POSITIVE', 'score': 0.9998804330825806}]\n",
      "\n",
      "Text: This was a terrible waste of time.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9998123049736023}]\n",
      "\n",
      "Text: Not bad, but could be better.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.8015893697738647}]\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "texts = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible waste of time.\",\n",
    "    \"Not bad, but could be better.\",\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(f\"\\nText: {t}\")\n",
    "    print(\"Prediction:\", clf(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a7855",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ –ó–∞–º–µ—Ä —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf5be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: 267.9 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"model_fp32.pt\")\n",
    "size_fp32 = os.path.getsize(\"model_fp32.pt\") / 1e6\n",
    "print(f\" –†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: {size_fp32:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d9110",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da26434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfd5885-4166-4999-9a14-c49e24e35b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: 138.7 MB\n",
      "–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: 48.2%\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(model, modules_to_quantize = {nn.Linear})\n",
    "size_quantized = save_model(quantized_model, \"model_quantized.pt\")\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {size_quantized:.1f} MB\")\n",
    "print(f\"–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {(1 - size_quantized/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eccfb8",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8406fcc-8424-4321-8b5c-5f79fd66db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [05:03<00:00,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Model Metrics:\n",
      "  accuracy: 0.9080\n",
      "  total_samples: 1000\n",
      "  correct_predictions: 908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating original model...\")\n",
    "original_metrics = evaluate_model_quality(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_samples=1000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Model Metrics:\")\n",
    "for key, value in original_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0af9d83-a14e-444d-bddf-0d58e6f3915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_quality(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    validation_dataset,\n",
    "    max_samples: int = 1000,\n",
    "    batch_size: int = 32,\n",
    "    max_length: int = 512\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        tokenizer: Tokenizer for the model\n",
    "        validation_dataset: Dataset with 'text' and 'label' fields\n",
    "        max_samples: Maximum number of samples to evaluate\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and other metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    eval_samples = min(max_samples, len(validation_dataset))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, eval_samples, batch_size), desc=\"Evaluating\"):\n",
    "            batch_end = min(i + batch_size, eval_samples)\n",
    "            batch = validation_dataset[i:batch_end]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch['text'],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = torch.tensor(batch['label']).to(device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    print(np.unique(all_labels))\n",
    "    \n",
    "    # For binary classification\n",
    "    if len(np.unique(all_labels)) == 2:\n",
    "        true_positives = np.sum((all_predictions == 1) & (all_labels == 1))\n",
    "        false_positives = np.sum((all_predictions == 1) & (all_labels == 0))\n",
    "        false_negatives = np.sum((all_predictions == 0) & (all_labels == 1))\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    else:\n",
    "        precision = recall = f1_score = None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'total_samples': total,\n",
    "        'correct_predictions': correct\n",
    "    }\n",
    "    \n",
    "    if precision is not None:\n",
    "        metrics.update({\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc78efa5-71c0-4a40-8576-85331c36562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "\n",
      "Original Model Metrics:\n",
      "  accuracy: 0.8750\n",
      "  total_samples: 64\n",
      "  correct_predictions: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating original model...\")\n",
    "original_metrics = evaluate_model_quality(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_samples=64,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Model Metrics:\")\n",
    "for key, value in original_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d23200-c716-4555-8ed2-e97b69d4a0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a2200-64de-49f8-ab54-7a5bfb38cc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe723e-3972-4674-b9f9-9081ba2d4c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9182c9-0157-42fd-ac35-ff4bce29f5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3705682a-5118-40f9-a839-383f15e539c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [04:05<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized Model Metrics:\n",
      "  accuracy: 0.8950\n",
      "  total_samples: 1000\n",
      "  correct_predictions: 895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating quantized model...\")\n",
    "quantized_metrics = evaluate_model_quality(\n",
    "    quantized_model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_samples=1000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\nQuantized Model Metrics:\")\n",
    "for key, value in quantized_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614fc8d6-e8d7-4ecd-818f-e3f99675131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy drop: 1.30%\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracy\n",
    "accuracy_drop = (original_metrics['accuracy'] - quantized_metrics['accuracy']) * 100\n",
    "print(f\"\\nAccuracy drop: {accuracy_drop:.2f}%\")\n",
    "\n",
    "if 'f1_score' in original_metrics:\n",
    "    f1_drop = (original_metrics['f1_score'] - quantized_metrics['f1_score']) * 100\n",
    "    print(f\"F1 Score drop: {f1_drop:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417e241",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ (–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ) –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28724b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference time for original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring inference time: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Original Model Inference Time:\n",
      "  Mean: 15.93 ms\n",
      "  Std:  0.54 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Measuring inference time for original model...\")\n",
    "original_time = measure_inference_time(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    num_runs=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö° Original Model Inference Time:\")\n",
    "print(f\"  Mean: {original_time['mean_time']*1000:.2f} ms\")\n",
    "print(f\"  Std:  {original_time['std_time']*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee46a79-f099-4a42-a334-1e292179e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference time for quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring inference time: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 34.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Quantized Model Inference Time:\n",
      "  Mean: 9.24 ms\n",
      "  Std:  2.47 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Measuring inference time for quantized model...\")\n",
    "quantized_time = measure_inference_time(\n",
    "    quantized_model,\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    num_runs=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö° Quantized Model Inference Time:\")\n",
    "print(f\"  Mean: {quantized_time['mean_time']*1000:.2f} ms\")\n",
    "print(f\"  Std:  {quantized_time['std_time']*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1fd1b1a-acf7-47e9-bf87-901246999c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Speedup: 1.72x\n"
     ]
    }
   ],
   "source": [
    "speedup = original_time['mean_time'] / quantized_time['mean_time']\n",
    "print(f\"\\nüöÄ Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f775132-5b07-42ad-b6be-29a1642f6524",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22dda480-2b74-42b7-a9d1-87c1f08715f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I loved this movie, it was fantastic!\n",
      "Prediction: [{'label': 'POSITIVE', 'score': 0.9998642206192017}]\n",
      "\n",
      "Text: This was a terrible waste of time.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9997791647911072}]\n",
      "\n",
      "Text: Not bad, but could be better.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9467726945877075}]\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "for t in texts:\n",
    "    print(f\"\\nText: {t}\")\n",
    "    print(\"Prediction:\", clf(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba5820-325c-4bb2-8687-07c7fc1debe6",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è post-training dynamic quantization:\n",
    "\n",
    "1. **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏** —É–º–µ–Ω—å—à–∏–ª—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 2 —Ä–∞–∑–∞ (—Å ~268 MB –¥–æ ~138 MB)\n",
    "2. **Accurcay**: –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 1.5%\n",
    "3. **–£—Å–∫–æ—Ä–µ–Ω–∏–µ** –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU - 1.72x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948ce97-1f95-447d-891d-2b22e5882f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "tmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
