{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "787b7284",
   "metadata": {},
   "source": [
    "# HT 2 : –°–∂–∞—Ç–∏–µ –º–æ–¥–µ–ª–∏ DistilBERT\n",
    "---\n",
    "–ó–∞–¥–∞—á–∞: —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤¬†2 —Ä–∞–∑–∞ —Å¬†–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534d1be",
   "metadata": {},
   "source": [
    "### üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cb7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa310d1",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116f9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch, os\n",
    "\n",
    "MODEL_ID = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"test\", )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eede46",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f7f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I loved this movie, it was fantastic!\n",
      "Prediction: [{'label': 'POSITIVE', 'score': 0.9998804330825806}]\n",
      "\n",
      "Text: This was a terrible waste of time.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9998123049736023}]\n",
      "\n",
      "Text: Not bad, but could be better.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.8015893697738647}]\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "texts = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible waste of time.\",\n",
    "    \"Not bad, but could be better.\",\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(f\"\\nText: {t}\")\n",
    "    print(\"Prediction:\", clf(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a7855",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ –ó–∞–º–µ—Ä —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf5be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " –†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: 267.9 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), \"model_fp32.pt\")\n",
    "size_fp32 = os.path.getsize(\"model_fp32.pt\") / 1e6\n",
    "print(f\" –†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: {size_fp32:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d9110",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da26434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfd5885-4166-4999-9a14-c49e24e35b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–∞–∑–º–µ—Ä –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: 138.7 MB\n",
      "–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: 48.2%\n"
     ]
    }
   ],
   "source": [
    "quantized_model = quantize_model(model, modules_to_quantize = {nn.Linear})\n",
    "size_quantized = save_model(quantized_model, \"model_quantized.pt\")\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {size_quantized:.1f} MB\")\n",
    "print(f\"–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {(1 - size_quantized/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eccfb8",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8406fcc-8424-4321-8b5c-5f79fd66db92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [05:16<00:00,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "\n",
      "Original Model Metrics:\n",
      "  accuracy: 0.8810\n",
      "  total_samples: 1000\n",
      "  correct_predictions: 881\n",
      "  precision: 0.8917\n",
      "  recall: 0.8607\n",
      "  f1_score: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating original model...\")\n",
    "original_metrics = evaluate_model_quality(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_samples=1000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Model Metrics:\")\n",
    "for key, value in original_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3705682a-5118-40f9-a839-383f15e539c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [03:31<00:00,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "\n",
      "Quantized Model Metrics:\n",
      "  accuracy: 0.8620\n",
      "  total_samples: 1000\n",
      "  correct_predictions: 862\n",
      "  precision: 0.8692\n",
      "  recall: 0.8443\n",
      "  f1_score: 0.8565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating quantized model...\")\n",
    "quantized_metrics = evaluate_model_quality(\n",
    "    quantized_model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    max_samples=1000,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(\"\\nQuantized Model Metrics:\")\n",
    "for key, value in quantized_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "614fc8d6-e8d7-4ecd-818f-e3f99675131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy drop: 1.90%\n",
      "F1 Score drop: 1.94%\n"
     ]
    }
   ],
   "source": [
    "accuracy_drop = (original_metrics['accuracy'] - quantized_metrics['accuracy']) * 100\n",
    "print(f\"\\nAccuracy drop: {accuracy_drop:.2f}%\")\n",
    "\n",
    "if 'f1_score' in original_metrics:\n",
    "    f1_drop = (original_metrics['f1_score'] - quantized_metrics['f1_score']) * 100\n",
    "    print(f\"F1 Score drop: {f1_drop:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417e241",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ (–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ) –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28724b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference time for original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring inference time: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Original Model Inference Time:\n",
      "  Mean: 15.93 ms\n",
      "  Std:  0.54 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Measuring inference time for original model...\")\n",
    "original_time = measure_inference_time(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    num_runs=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö° Original Model Inference Time:\")\n",
    "print(f\"  Mean: {original_time['mean_time']*1000:.2f} ms\")\n",
    "print(f\"  Std:  {original_time['std_time']*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee46a79-f099-4a42-a334-1e292179e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference time for quantized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring inference time: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:02<00:00, 34.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Quantized Model Inference Time:\n",
      "  Mean: 9.24 ms\n",
      "  Std:  2.47 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Measuring inference time for quantized model...\")\n",
    "quantized_time = measure_inference_time(\n",
    "    quantized_model,\n",
    "    tokenizer,\n",
    "    texts,\n",
    "    num_runs=100\n",
    ")\n",
    "\n",
    "print(\"\\n‚ö° Quantized Model Inference Time:\")\n",
    "print(f\"  Mean: {quantized_time['mean_time']*1000:.2f} ms\")\n",
    "print(f\"  Std:  {quantized_time['std_time']*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1fd1b1a-acf7-47e9-bf87-901246999c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Speedup: 1.72x\n"
     ]
    }
   ],
   "source": [
    "speedup = original_time['mean_time'] / quantized_time['mean_time']\n",
    "print(f\"\\nüöÄ Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f775132-5b07-42ad-b6be-29a1642f6524",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22dda480-2b74-42b7-a9d1-87c1f08715f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I loved this movie, it was fantastic!\n",
      "Prediction: [{'label': 'POSITIVE', 'score': 0.9998642206192017}]\n",
      "\n",
      "Text: This was a terrible waste of time.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9997791647911072}]\n",
      "\n",
      "Text: Not bad, but could be better.\n",
      "Prediction: [{'label': 'NEGATIVE', 'score': 0.9467726945877075}]\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"sentiment-analysis\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "for t in texts:\n",
    "    print(f\"\\nText: {t}\")\n",
    "    print(\"Prediction:\", clf(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba5820-325c-4bb2-8687-07c7fc1debe6",
   "metadata": {},
   "source": [
    "### –í—ã–≤–æ–¥—ã\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è post-training dynamic quantization:\n",
    "\n",
    "1. **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏** —É–º–µ–Ω—å—à–∏–ª—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 2 —Ä–∞–∑–∞ (—Å ~268 MB –¥–æ ~138 MB)\n",
    "2. **Accurcay**: –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 1.9%\n",
    "3. **–£—Å–∫–æ—Ä–µ–Ω–∏–µ** –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ CPU - 1.72x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9491071-be9a-4dd5-848a-af630e3976b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "tmp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
